{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7ed452fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module implements training and evaluation of a multi-layer perceptron in PyTorch.\n",
    "You should fill in code into indicated sections.\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "import tensorboard as tb\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c9a71800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b55d92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, n_inputs, n_hidden, n_classes, use_batch_norm=False):\n",
    "        \"\"\" Initializing the MLP module \"\"\"\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        dims = n_inputs + n_hidden + n_classes\n",
    "\n",
    "        for i in range(len(dims) - 1):\n",
    "            fan_in, fan_out = dims[i], dims[i+1]\n",
    "\n",
    "            lin = nn.Linear(fan_in, fan_out, bias=True)\n",
    "            nn.init.kaiming_normal_(lin.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "            nn.init.zeros_(lin.bias)\n",
    "            layers.append(lin)\n",
    "\n",
    "            if i < len(dims) - 2:\n",
    "                if use_batch_norm:\n",
    "                    layers.append(nn.BatchNorm1d(fan_out))\n",
    "                layers.append(nn.ReLU())\n",
    "\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        out = self.net(x)\n",
    "        return out\n",
    "    \n",
    "    @property\n",
    "    def device(self):\n",
    "        \"\"\"\n",
    "        Returns the device on which the model is. Can be useful in some situations.\n",
    "        \"\"\"\n",
    "        return next(self.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4cfeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): # GPU operation have separate seed\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Additionally, some operations on a GPU are implemented stochastic for efficiency\n",
    "# We want to ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Fetching the device that will be used throughout this notebook\n",
    "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
    "print(\"Using device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbd091d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, targets):\n",
    "\n",
    "    preds = predictions.argmax(dim=1)\n",
    "    correct = (preds == targets)\n",
    "    accuracy = correct.float().mean().item()\n",
    "    return accuracy\n",
    "\n",
    "def eval_model(model, data_loader):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            preds = outputs.argmax(dim=1)\n",
    "\n",
    "            total_correct += (preds == targets).sum().item()\n",
    "            total_samples += targets.size(0)\n",
    "\n",
    "    avg_accuracy = total_correct / total_samples\n",
    "\n",
    "    return avg_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435f38ee",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d38a707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n"
     ]
    }
   ],
   "source": [
    "# Path to the folder where the datasets are/should be downloaded (e.g. MNIST)\n",
    "DATASET_PATH = \"../data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"../saved_models/tutorial3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0cf4a543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "# Transformations applied on each image => first make them a tensor, then normalize them in the range -1 to 1\n",
    "transform = transforms.Compose([transforms.ToTensor(), \n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Loading the training dataset. We need to split it into a training and validation part\n",
    "train_dataset = FashionMNIST(root=DATASET_PATH, train=True, transform=transform, download=True)\n",
    "train_set, val_set = torch.utils.data.random_split(train_dataset, [50000, 10000])\n",
    "\n",
    "# Loading the test set\n",
    "test_set = FashionMNIST(root=DATASET_PATH, train=False, transform=transform, download=True)\n",
    "\n",
    "# We define a set of data loaders that we can use for various purposes later.\n",
    "# Note that for actually training a model, we will use different data loaders\n",
    "# with a lower batch size.\n",
    "train_loader = data.DataLoader(train_set, batch_size=1024, shuffle=True, drop_last=False)\n",
    "val_loader = data.DataLoader(val_set, batch_size=1024, shuffle=False, drop_last=False)\n",
    "test_loader = data.DataLoader(test_set, batch_size=1024, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc01cc0",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4821bf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    batch_size,\n",
    "    epochs,\n",
    "    seed,\n",
    "    lr,\n",
    "    device,\n",
    "    data_dir=\"runs/fashionmnist_experiment\"\n",
    "):  \n",
    "    # seed for reproducability\n",
    "    set_seed(seed)\n",
    "    # tensorboard\n",
    "    writer = SummaryWriter(data_dir)\n",
    "    model_plotted = False\n",
    "\n",
    "    # loss module and optimizer\n",
    "    loss_module = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # initializing best tracking\n",
    "    best_val_acc = -1.0\n",
    "    best_epoch = -1\n",
    "    best_model = deepcopy(model)\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            # push to device\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            if not model_plotted:\n",
    "                writer.add_graph(model, inputs)\n",
    "                model_plotted = True\n",
    "            # set gradient to nothing\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            logits = model(inputs)\n",
    "            loss = loss_module(logits, targets)\n",
    "\n",
    "            # backward + update\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # stats\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # predictions is the output of the system\n",
    "            preds = logits.argmax(dim=1)\n",
    "            total_correct += (preds == targets).sum().item()\n",
    "            total_samples += targets.size(0)\n",
    "\n",
    "        # epoch metrics (outside batch loop!)\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = total_correct / total_samples\n",
    "\n",
    "        # write to tensorboard\n",
    "        writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
    "        writer.add_scalar(\"Accuracy/train\", epoch_acc, epoch)\n",
    "\n",
    "        # validation\n",
    "        val_acc = eval_model(model, test_loader)  # uses no_grad + eval inside\n",
    "        writer.add_scalar(\"Accuracy/val\", val_acc, epoch)\n",
    "        \n",
    "        # checkpoint\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"val_acc\": val_acc,\n",
    "        }, \"checkpoint.pth\")\n",
    "\n",
    "        # track best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = epoch\n",
    "            best_model = deepcopy(model)\n",
    "\n",
    "    # Test best model\n",
    "    best_test_acc = eval_model(best_model, test_loader)\n",
    "    writer.add_scalar(\"Accuracy/best_test\", best_test_acc, epochs)\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "    torch.save(best_model.state_dict(), \"best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "27eb6f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b86f52d4bd4429964a0ffdf09ed138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 | train_loss=1.1724 | train_acc=0.5751 | val_acc=0.6235 | best_val_acc=0.6235 (epoch 1)\n",
      "Epoch 2/2 | train_loss=0.5558 | train_acc=0.7956 | val_acc=0.8225 | best_val_acc=0.8225 (epoch 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(MLP(\n",
       "   (net): Sequential(\n",
       "     (0): Linear(in_features=784, out_features=256, bias=True)\n",
       "     (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): SiLU()\n",
       "     (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (5): SiLU()\n",
       "     (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (7): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (8): SiLU()\n",
       "     (9): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (10): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (11): SiLU()\n",
       "     (12): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (13): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (14): SiLU()\n",
       "     (15): Linear(in_features=256, out_features=128, bias=True)\n",
       "     (16): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (17): SiLU()\n",
       "     (18): Linear(in_features=128, out_features=128, bias=True)\n",
       "     (19): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (20): SiLU()\n",
       "     (21): Linear(in_features=128, out_features=128, bias=True)\n",
       "     (22): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (23): SiLU()\n",
       "     (24): Linear(in_features=128, out_features=128, bias=True)\n",
       "     (25): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (26): SiLU()\n",
       "     (27): Linear(in_features=128, out_features=10, bias=True)\n",
       "   )\n",
       " ),\n",
       " 0.8225,\n",
       " 1,\n",
       " 0.8225)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP(n_inputs=[784], \n",
    "            n_hidden=[256, 256, 256, 256, 256, 128, 128, 128, 128], \n",
    "            n_classes=[10], use_batch_norm=True)\n",
    "\n",
    "train_model(model=model, \n",
    "            train_loader=train_loader,\n",
    "            test_loader=test_loader,\n",
    "            device=device,\n",
    "            batch_size=1024,\n",
    "            epochs=2,\n",
    "            seed=42,\n",
    "            lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9768a749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6009 (pid 91744), started 0:17:57 ago. (Use '!kill 91744' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-e7b9c50dcaafae61\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-e7b9c50dcaafae61\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6009;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir runs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
