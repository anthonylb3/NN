{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "650f987c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# MIT License\n",
    "#\n",
    "# Copyright (c) 2025 University of Amsterdam\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to conditions.\n",
    "#\n",
    "# Author: Deep Learning Course (UvA) | Fall 2025\n",
    "# Date Created: 2025-10-28\n",
    "################################################################################\n",
    "\"\"\"\n",
    "This module implements training and evaluation of a multi-layer perceptron in PyTorch.\n",
    "You should fill in code into indicated sections.\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from tqdm.auto import tqdm\n",
    "from mlp_pytorch import MLP\n",
    "import cifar10_utils\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9f8af6",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc1bea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, targets):\n",
    "    \"\"\"\n",
    "    Computes the prediction accuracy, i.e. the average of correct predictions\n",
    "    of the network.\n",
    "\n",
    "    Args:\n",
    "      predictions: 2D float array of size [batch_size, n_classes], predictions of the model (logits)\n",
    "      llabels: 1D int array of size [batch_size]. Ground truth labels for\n",
    "               each sample in the batch\n",
    "    Returns:\n",
    "      accuracy: scalar float, the accuracy of predictions,\n",
    "                i.e. the average correct predictions over the whole batch\n",
    "\n",
    "    TODO:\n",
    "    Implement accuracy computation.\n",
    "    \"\"\"\n",
    "\n",
    "    #######################\n",
    "    # PUT YOUR CODE HERE  #\n",
    "    #######################\n",
    "    # Stap 1: voorspelde klasse = index van hoogste logit\n",
    "    preds = predictions.argmax(dim=1)\n",
    "\n",
    "    # Stap 2: vergelijk met targets\n",
    "    correct = (preds == targets)\n",
    "\n",
    "    # Stap 3: gemiddelde nemen (float)\n",
    "    accuracy = correct.float().mean().item()\n",
    "\n",
    "    #######################\n",
    "    # END OF YOUR CODE    #\n",
    "    #######################\n",
    "\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed5f7c5",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c0f3e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader):\n",
    "    \"\"\"\n",
    "    Performs the evaluation of the MLP model on a given dataset.\n",
    "\n",
    "    Args:\n",
    "      model: An instance of 'MLP', the model to evaluate.\n",
    "      data_loader: The data loader of the dataset to evaluate.\n",
    "    Returns:\n",
    "      avg_accuracy: scalar float, the average accuracy of the model on the dataset.\n",
    "\n",
    "    TODO:\n",
    "    Implement evaluation of the MLP model on a given dataset.\n",
    "\n",
    "    Hint: make sure to return the average accuracy of the whole dataset,\n",
    "          independent of batch sizes (not all batches might be the same size).\n",
    "    \"\"\"\n",
    "\n",
    "    #######################\n",
    "    # PUT YOUR CODE HERE  #\n",
    "    #######################\n",
    "    model.eval()  # zet model in evaluation mode (belangrijk voor BatchNorm)\n",
    "\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():  # geen gradients nodig tijdens evaluatie\n",
    "        for inputs, targets in data_loader:\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            preds = outputs.argmax(dim=1)\n",
    "\n",
    "            total_correct += (preds == targets).sum().item()\n",
    "            total_samples += targets.size(0)\n",
    "\n",
    "    avg_accuracy = total_correct / total_samples\n",
    "    #######################\n",
    "    # END OF YOUR CODE    #\n",
    "    #######################\n",
    "\n",
    "    return avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f416115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(hidden_dims, lr, use_batch_norm, batch_size, epochs, seed, data_dir):\n",
    "    \"\"\"\n",
    "    Performs a full training cycle of MLP model.\n",
    "\n",
    "    Args:\n",
    "      hidden_dims: A list of ints, specificying the hidden dimensionalities to use in the MLP.\n",
    "      lr: Learning rate of the SGD to apply.\n",
    "      use_batch_norm: If True, adds batch normalization layer into the network.\n",
    "      batch_size: Minibatch size for the data loaders.\n",
    "      epochs: Number of training epochs to perform.\n",
    "      seed: Seed to use for reproducible results.\n",
    "      data_dir: Directory where to store/find the CIFAR10 dataset.\n",
    "    Returns:\n",
    "      model: An instance of 'MLP', the trained model that performed best on the validation set.\n",
    "      val_accuracies: A list of scalar floats, containing the accuracies of the model on the\n",
    "                      validation set per epoch (element 0 - performance after epoch 1)\n",
    "      test_accuracy: scalar float, average accuracy on the test dataset of the model that\n",
    "                     performed best on the validation.\n",
    "      logging_dict: An arbitrary object containing logging information. This is for you to\n",
    "                    decide what to put in here.\n",
    "\n",
    "    TODO:\n",
    "    - Implement the training of the MLP model.\n",
    "    - Evaluate your model on the whole validation set each epoch.\n",
    "    - After finishing training, evaluate your model that performed best on the validation set,\n",
    "      on the whole test dataset.\n",
    "    - Integrate _all_ input arguments of this function in your training. You are allowed to add\n",
    "      additional input argument if you assign it a default value that represents the plain training\n",
    "      (e.g. '..., new_param=False')\n",
    "\n",
    "    Hint: you can save your best model by deepcopy-ing it.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the random seeds for reproducibility\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():  # GPU operation have separate seed\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.determinstic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Set default device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Loading the dataset\n",
    "    cifar10 = cifar10_utils.get_cifar10(data_dir)\n",
    "    cifar10_loader = cifar10_utils.get_dataloader(cifar10, batch_size=batch_size,\n",
    "                                                  return_numpy=False)\n",
    "\n",
    "    #######################\n",
    "    # PUT YOUR CODE HERE  #\n",
    "    #######################\n",
    "    # --- Get train/val/test loaders (works if returned as tuple or dict) ---\n",
    "    train_loader = cifar10_loader[\"train\"]\n",
    "    val_loader   = cifar10_loader[\"validation\"]\n",
    "    test_loader  = cifar10_loader[\"test\"]\n",
    "\n",
    "    if val_loader is None:\n",
    "        val_loader = test_loader\n",
    "\n",
    "\n",
    "    # TODO: Initialize model and loss module\n",
    "    n_inputs = 3 * 32 * 32\n",
    "    n_classes = 10\n",
    "\n",
    "    model = MLP(n_inputs=n_inputs, n_hidden=hidden_dims, n_classes=n_classes,\n",
    "                use_batch_norm=use_batch_norm).to(device)\n",
    "    loss_module = nn.CrossEntropyLoss()\n",
    "\n",
    "    # TODO: Do optimization with the simple SGD optimizer\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    val_accuracies = []\n",
    "    # TODO: Training loop including validation\n",
    "    train_losses = []\n",
    "\n",
    "    best_val_acc = -1.0\n",
    "    best_epoch = -1\n",
    "    best_model = deepcopy(model)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        n_seen = 0\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # CIFAR10 images -> flatten to vectors\n",
    "            inputs = inputs.view(inputs.size(0), -1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits = model(inputs)\n",
    "            loss = loss_module(logits, targets)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            bs = targets.size(0)\n",
    "            running_loss += loss.item() * bs\n",
    "            n_seen += bs\n",
    "\n",
    "        avg_train_loss = running_loss / max(1, n_seen)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # --- Validation accuracy on full validation set (size-weighted) ---\n",
    "        model.eval()\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "                inputs = inputs.view(inputs.size(0), -1)\n",
    "\n",
    "                logits = model(inputs)\n",
    "                preds = logits.argmax(dim=1)\n",
    "\n",
    "                total_correct += (preds == targets).sum().item()\n",
    "                total_samples += targets.size(0)\n",
    "\n",
    "        val_acc = total_correct / max(1, total_samples)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        # --- Save best model (by validation accuracy) ---\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = epoch\n",
    "            best_model = deepcopy(model)\n",
    "\n",
    "    # TODO: Test best model\n",
    "    best_model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            inputs = inputs.view(inputs.size(0), -1)\n",
    "\n",
    "            logits = best_model(inputs)\n",
    "            preds = logits.argmax(dim=1)\n",
    "\n",
    "            total_correct += (preds == targets).sum().item()\n",
    "            total_samples += targets.size(0)\n",
    "\n",
    "    test_accuracy = total_correct / max(1, total_samples)\n",
    "\n",
    "    # TODO: Add any information you might want to save for plotting\n",
    "    # --- Logging (you can add more if you want) ---\n",
    "    logging_dict = {\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_accuracies\": val_accuracies,\n",
    "        \"best_val_accuracy\": best_val_acc,\n",
    "        \"best_epoch\": best_epoch,\n",
    "        \"lr\": lr,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"hidden_dims\": hidden_dims,\n",
    "        \"use_batch_norm\": use_batch_norm,\n",
    "        \"seed\": seed,\n",
    "        \"device\": str(device),\n",
    "    }\n",
    "\n",
    "    #######################\n",
    "    # END OF YOUR CODE    #\n",
    "    #######################\n",
    "\n",
    "    return model, val_accuracies, test_accuracy, logging_dict\n",
    "\n",
    "# Feel free to add any additional functions, such as plotting of the loss curve here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9643754",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 30] Read-only file system: '/data'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model, val_accuracies, test_accuracy, logging_dict = train(hidden_dims=\u001b[32m128\u001b[39m,\n\u001b[32m      2\u001b[39m                                                            use_batch_norm=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      3\u001b[39m                                                            lr=\u001b[32m0.1\u001b[39m,\n\u001b[32m      4\u001b[39m                                                            batch_size=\u001b[32m128\u001b[39m,\n\u001b[32m      5\u001b[39m                                                            epochs=\u001b[32m10\u001b[39m,\n\u001b[32m      6\u001b[39m                                                            seed=\u001b[32m42\u001b[39m,\n\u001b[32m      7\u001b[39m                                                            data_dir=\u001b[33m\"\u001b[39m\u001b[33m/data\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m torch.save(model.state_dict(), \u001b[33m\"\u001b[39m\u001b[33mbest_model.pth\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(logging_dict)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(hidden_dims, lr, use_batch_norm, batch_size, epochs, seed, data_dir)\u001b[39m\n\u001b[32m     44\u001b[39m device = torch.device(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Loading the dataset\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m cifar10 = cifar10_utils.get_cifar10(data_dir)\n\u001b[32m     48\u001b[39m cifar10_loader = cifar10_utils.get_dataloader(cifar10, batch_size=batch_size,\n\u001b[32m     49\u001b[39m                                               return_numpy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m#######################\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# PUT YOUR CODE HERE  #\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m#######################\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# --- Get train/val/test loaders (works if returned as tuple or dict) ---\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NN/assignment1_NN/src/cifar10_utils.py:93\u001b[39m, in \u001b[36mget_cifar10\u001b[39m\u001b[34m(data_dir, validation_size)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_cifar10\u001b[39m(data_dir=\u001b[33m'\u001b[39m\u001b[33mdata/\u001b[39m\u001b[33m'\u001b[39m, validation_size=\u001b[32m5000\u001b[39m):\n\u001b[32m     84\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[33;03m    Prepares CIFAR10 dataset.\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     91\u001b[39m \u001b[33;03m      Dictionary with Train, Validation, Test Datasets\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m read_data_sets(data_dir, validation_size)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NN/assignment1_NN/src/cifar10_utils.py:68\u001b[39m, in \u001b[36mread_data_sets\u001b[39m\u001b[34m(data_dir, validation_size)\u001b[39m\n\u001b[32m     61\u001b[39m std  = (\u001b[32m0.247\u001b[39m, \u001b[32m0.243\u001b[39m, \u001b[32m0.262\u001b[39m)\n\u001b[32m     63\u001b[39m data_transforms = transforms.Compose([\n\u001b[32m     64\u001b[39m                         transforms.ToTensor(),\n\u001b[32m     65\u001b[39m                         transforms.Normalize(mean, std)\n\u001b[32m     66\u001b[39m                     ])\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m train_dataset = CIFAR10(root=data_dir, train=\u001b[38;5;28;01mTrue\u001b[39;00m, download=\u001b[38;5;28;01mTrue\u001b[39;00m, transform=data_transforms)\n\u001b[32m     69\u001b[39m test_dataset = CIFAR10(root=data_dir, train=\u001b[38;5;28;01mFalse\u001b[39;00m, download=\u001b[38;5;28;01mTrue\u001b[39;00m, transform=data_transforms)\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# Subsample the validation set from the train set\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/dl2025/lib/python3.12/site-packages/torchvision/datasets/cifar.py:66\u001b[39m, in \u001b[36mCIFAR10.__init__\u001b[39m\u001b[34m(self, root, train, transform, target_transform, download)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28mself\u001b[39m.train = train  \u001b[38;5;66;03m# training set or test set\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     \u001b[38;5;28mself\u001b[39m.download()\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._check_integrity():\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/dl2025/lib/python3.12/site-packages/torchvision/datasets/cifar.py:140\u001b[39m, in \u001b[36mCIFAR10.download\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    138\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFiles already downloaded and verified\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m download_and_extract_archive(\u001b[38;5;28mself\u001b[39m.url, \u001b[38;5;28mself\u001b[39m.root, filename=\u001b[38;5;28mself\u001b[39m.filename, md5=\u001b[38;5;28mself\u001b[39m.tgz_md5)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/dl2025/lib/python3.12/site-packages/torchvision/datasets/utils.py:395\u001b[39m, in \u001b[36mdownload_and_extract_archive\u001b[39m\u001b[34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[39m\n\u001b[32m    392\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filename:\n\u001b[32m    393\u001b[39m     filename = os.path.basename(url)\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m download_url(url, download_root, filename, md5)\n\u001b[32m    397\u001b[39m archive = os.path.join(download_root, filename)\n\u001b[32m    398\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExtracting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marchive\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextract_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/dl2025/lib/python3.12/site-packages/torchvision/datasets/utils.py:111\u001b[39m, in \u001b[36mdownload_url\u001b[39m\u001b[34m(url, root, filename, md5, max_redirect_hops)\u001b[39m\n\u001b[32m    108\u001b[39m     filename = os.path.basename(url)\n\u001b[32m    109\u001b[39m fpath = os.fspath(os.path.join(root, filename))\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m os.makedirs(root, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# check if file is already present locally\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_integrity(fpath, md5):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:225\u001b[39m, in \u001b[36mmakedirs\u001b[39m\u001b[34m(name, mode, exist_ok)\u001b[39m\n",
      "\u001b[31mOSError\u001b[39m: [Errno 30] Read-only file system: '/data'"
     ]
    }
   ],
   "source": [
    "model, val_accuracies, test_accuracy, logging_dict = train(hidden_dims=128,\n",
    "                                                           use_batch_norm=True,\n",
    "                                                           lr=0.1,\n",
    "                                                           batch_size=128,\n",
    "                                                           epochs=10,\n",
    "                                                           seed=42,\n",
    "                                                           data_dir=\"/data\")\n",
    "\n",
    "torch.save(model.state_dict(), \"best_model.pth\")\n",
    "print(logging_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480bf015",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir runs/our_experiment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
