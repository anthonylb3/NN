{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650f987c",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# MIT License\n",
    "#\n",
    "# Copyright (c) 2025 University of Amsterdam\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to conditions.\n",
    "#\n",
    "# Author: Deep Learning Course (UvA) | Fall 2025\n",
    "# Date Created: 2025-10-28\n",
    "################################################################################\n",
    "\"\"\"\n",
    "This module implements training and evaluation of a multi-layer perceptron in PyTorch.\n",
    "You should fill in code into indicated sections.\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from tqdm.auto import tqdm\n",
    "from mlp_pytorch import MLP\n",
    "import cifar10_utils\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tensorboard as tf\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9f8af6",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc1bea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, targets):\n",
    "    \"\"\"\n",
    "    Computes the prediction accuracy, i.e. the average of correct predictions\n",
    "    of the network.\n",
    "\n",
    "    Args:\n",
    "      predictions: 2D float array of size [batch_size, n_classes], predictions of the model (logits)\n",
    "      llabels: 1D int array of size [batch_size]. Ground truth labels for\n",
    "               each sample in the batch\n",
    "    Returns:\n",
    "      accuracy: scalar float, the accuracy of predictions,\n",
    "                i.e. the average correct predictions over the whole batch\n",
    "\n",
    "    TODO:\n",
    "    Implement accuracy computation.\n",
    "    \"\"\"\n",
    "\n",
    "    #######################\n",
    "    # PUT YOUR CODE HERE  #\n",
    "    #######################\n",
    "    # Stap 1: voorspelde klasse = index van hoogste logit\n",
    "    preds = predictions.argmax(dim=1)\n",
    "\n",
    "    # Stap 2: vergelijk met targets\n",
    "    correct = (preds == targets)\n",
    "\n",
    "    # Stap 3: gemiddelde nemen (float)\n",
    "    accuracy = correct.float().mean().item()\n",
    "\n",
    "    #######################\n",
    "    # END OF YOUR CODE    #\n",
    "    #######################\n",
    "\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed5f7c5",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c0f3e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader):\n",
    "    \"\"\"\n",
    "    Performs the evaluation of the MLP model on a given dataset.\n",
    "\n",
    "    Args:\n",
    "      model: An instance of 'MLP', the model to evaluate.\n",
    "      data_loader: The data loader of the dataset to evaluate.\n",
    "    Returns:\n",
    "      avg_accuracy: scalar float, the average accuracy of the model on the dataset.\n",
    "\n",
    "    TODO:\n",
    "    Implement evaluation of the MLP model on a given dataset.\n",
    "\n",
    "    Hint: make sure to return the average accuracy of the whole dataset,\n",
    "          independent of batch sizes (not all batches might be the same size).\n",
    "    \"\"\"\n",
    "\n",
    "    #######################\n",
    "    # PUT YOUR CODE HERE  #\n",
    "    #######################\n",
    "    model.eval()  # zet model in evaluation mode (belangrijk voor BatchNorm)\n",
    "\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():  # geen gradients nodig tijdens evaluatie\n",
    "        for inputs, targets in data_loader:\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            preds = outputs.argmax(dim=1)\n",
    "\n",
    "            total_correct += (preds == targets).sum().item()\n",
    "            total_samples += targets.size(0)\n",
    "\n",
    "    avg_accuracy = total_correct / total_samples\n",
    "    #######################\n",
    "    # END OF YOUR CODE    #\n",
    "    #######################\n",
    "\n",
    "    return avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f416115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(hidden_dims, lr, use_batch_norm, batch_size, epochs, seed, data_dir):\n",
    "    \"\"\"\n",
    "    Performs a full training cycle of MLP model.\n",
    "\n",
    "    Args:\n",
    "      hidden_dims: A list of ints, specificying the hidden dimensionalities to use in the MLP.\n",
    "      lr: Learning rate of the SGD to apply.\n",
    "      use_batch_norm: If True, adds batch normalization layer into the network.\n",
    "      batch_size: Minibatch size for the data loaders.\n",
    "      epochs: Number of training epochs to perform.\n",
    "      seed: Seed to use for reproducible results.\n",
    "      data_dir: Directory where to store/find the CIFAR10 dataset.\n",
    "    Returns:\n",
    "      model: An instance of 'MLP', the trained model that performed best on the validation set.\n",
    "      val_accuracies: A list of scalar floats, containing the accuracies of the model on the\n",
    "                      validation set per epoch (element 0 - performance after epoch 1)\n",
    "      test_accuracy: scalar float, average accuracy on the test dataset of the model that\n",
    "                     performed best on the validation.\n",
    "      logging_dict: An arbitrary object containing logging information. This is for you to\n",
    "                    decide what to put in here.\n",
    "\n",
    "    TODO:\n",
    "    - Implement the training of the MLP model.\n",
    "    - Evaluate your model on the whole validation set each epoch.\n",
    "    - After finishing training, evaluate your model that performed best on the validation set,\n",
    "      on the whole test dataset.\n",
    "    - Integrate _all_ input arguments of this function in your training. You are allowed to add\n",
    "      additional input argument if you assign it a default value that represents the plain training\n",
    "      (e.g. '..., new_param=False')\n",
    "\n",
    "    Hint: you can save your best model by deepcopy-ing it.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the random seeds for reproducibility\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():  # GPU operation have separate seed\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.determinstic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Set default device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Loading the dataset\n",
    "    cifar10 = cifar10_utils.get_cifar10(data_dir)\n",
    "    cifar10_loader = cifar10_utils.get_dataloader(cifar10, batch_size=batch_size,\n",
    "                                                  return_numpy=False)\n",
    "\n",
    "    #######################\n",
    "    # PUT YOUR CODE HERE  #\n",
    "    #######################\n",
    "    # --- Get train/val/test loaders (works if returned as tuple or dict) ---\n",
    "    train_loader = cifar10_loader[\"train\"]\n",
    "    val_loader   = cifar10_loader[\"validation\"]\n",
    "    test_loader  = cifar10_loader[\"test\"]\n",
    "\n",
    "    # TODO: Initialize model and loss module\n",
    "    n_inputs = 3 * 32 * 32\n",
    "    n_classes = 10\n",
    "\n",
    "    model = MLP(n_inputs=n_inputs, n_hidden=hidden_dims, n_classes=n_classes,\n",
    "                use_batch_norm=use_batch_norm).to(device)\n",
    "    loss_module = nn.CrossEntropyLoss()\n",
    "\n",
    "    # TODO: Do optimization with the simple SGD optimizer\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    val_accuracies = []\n",
    "    # TODO: Training loop including validation\n",
    "    train_losses = []\n",
    "\n",
    "    best_val_acc = -1.0\n",
    "    best_epoch = -1\n",
    "    best_model = deepcopy(model)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        n_seen = 0\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # CIFAR10 images -> flatten to vectors\n",
    "            inputs = inputs.view(inputs.size(0), -1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits = model(inputs)\n",
    "            loss = loss_module(logits, targets)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            bs = targets.size(0)\n",
    "            running_loss += loss.item() * bs\n",
    "            n_seen += bs\n",
    "\n",
    "        avg_train_loss = running_loss / max(1, n_seen)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # --- Validation accuracy on full validation set (size-weighted) ---\n",
    "        model.eval()\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "                inputs = inputs.view(inputs.size(0), -1)\n",
    "\n",
    "                logits = model(inputs)\n",
    "                preds = logits.argmax(dim=1)\n",
    "\n",
    "                total_correct += (preds == targets).sum().item()\n",
    "                total_samples += targets.size(0)\n",
    "\n",
    "        val_acc = total_correct / max(1, total_samples)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        # --- Save best model (by validation accuracy) ---\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = epoch\n",
    "            best_model = deepcopy(model)\n",
    "\n",
    "    # TODO: Test best model\n",
    "    best_model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            inputs = inputs.view(inputs.size(0), -1)\n",
    "\n",
    "            logits = best_model(inputs)\n",
    "            preds = logits.argmax(dim=1)\n",
    "\n",
    "            total_correct += (preds == targets).sum().item()\n",
    "            total_samples += targets.size(0)\n",
    "\n",
    "    test_accuracy = total_correct / max(1, total_samples)\n",
    "\n",
    "    # TODO: Add any information you might want to save for plotting\n",
    "    # --- Logging (you can add more if you want) ---\n",
    "    logging_dict = {\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_accuracies\": val_accuracies,\n",
    "        \"best_val_accuracy\": best_val_acc,\n",
    "        \"best_epoch\": best_epoch,\n",
    "        \"lr\": lr,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"hidden_dims\": hidden_dims,\n",
    "        \"use_batch_norm\": use_batch_norm,\n",
    "        \"seed\": seed,\n",
    "        \"device\": str(device),\n",
    "    }\n",
    "\n",
    "    #######################\n",
    "    # END OF YOUR CODE    #\n",
    "    #######################\n",
    "\n",
    "    return model, val_accuracies, test_accuracy, logging_dict\n",
    "\n",
    "# Feel free to add any additional functions, such as plotting of the loss curve here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9643754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/dl2025/lib/python3.12/site-packages/torchvision/datasets/cifar.py:83: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n",
      "  entry = pickle.load(f, encoding=\"latin1\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_dataloader() got an unexpected keyword argument 'shuffle'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model, val_accuracies, test_accuracy, logging_dict = train(hidden_dims=[\u001b[32m128\u001b[39m],\n\u001b[32m      2\u001b[39m                                                            use_batch_norm=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      3\u001b[39m                                                            lr=\u001b[32m0.1\u001b[39m,\n\u001b[32m      4\u001b[39m                                                            batch_size=\u001b[32m32\u001b[39m,\n\u001b[32m      5\u001b[39m                                                            epochs=\u001b[32m10\u001b[39m,\n\u001b[32m      6\u001b[39m                                                            seed=\u001b[32m42\u001b[39m,\n\u001b[32m      7\u001b[39m                                                            data_dir=\u001b[33m\"\u001b[39m\u001b[33mdata/\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m torch.save(model.state_dict(), \u001b[33m\"\u001b[39m\u001b[33mbest_model.pth\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(logging_dict)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(hidden_dims, lr, use_batch_norm, batch_size, epochs, seed, data_dir)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Loading the dataset\u001b[39;00m\n\u001b[32m     47\u001b[39m cifar10 = cifar10_utils.get_cifar10(data_dir)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m cifar10_loader = cifar10_utils.get_dataloader(cifar10, batch_size=batch_size,\n\u001b[32m     49\u001b[39m                                               return_numpy=\u001b[38;5;28;01mFalse\u001b[39;00m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m#######################\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# PUT YOUR CODE HERE  #\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m#######################\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# --- Get train/val/test loaders (works if returned as tuple or dict) ---\u001b[39;00m\n\u001b[32m     55\u001b[39m train_loader = cifar10_loader[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mTypeError\u001b[39m: get_dataloader() got an unexpected keyword argument 'shuffle'"
     ]
    }
   ],
   "source": [
    "model, val_accuracies, test_accuracy, logging_dict = train(hidden_dims=[128],\n",
    "                                                           use_batch_norm=True,\n",
    "                                                           lr=0.1,\n",
    "                                                           batch_size=32,\n",
    "                                                           epochs=10,\n",
    "                                                           seed=42,\n",
    "                                                           data_dir=\"data/\")\n",
    "\n",
    "torch.save(model.state_dict(), \"best_model.pth\")\n",
    "print(logging_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "480bf015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b6085f56e6c7b8d1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b6085f56e6c7b8d1\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir runs/our_experiment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
